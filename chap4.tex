\chapter{Arbres binaires de recherche ABR}

\section{Complexité}\index{Complexité}

Pour un algorithme $A$ on note $\text{coût}_{A}(x)$ le nombre d'opérations élémentaires que l'algorithme effectue sur une entrée $x$.

\textbf{Complexité au pire cas}\index{Complexité au pire cas} : $\text{coût}_{A}(n) = \max \text{coût}_{A}(x)$, $\,x$ de largeur $n$.

\textbf{Coût amorti}\index{Coût amorti} d'un algorithme $A$ : on exécute $A(x_{1})\, A(x_{2})\, ...\,\, A(x_{n})$; $\text{coût amorti} = \frac{1}{n}\sum\limits_{i = 1}^{n}\, \text{coût}_{A}(x_{i})$

\textbf{Complexité en moyenne}\index{Complexité en moyenne} : \textit{"définition mathématique formelle de, à la louche"}, on a une distinction sur les entrées $\mu$, $\text{coût}_{A}^{\mu}(x) =\sum\limits_{x\thicksim \mu} \mu(x)\, \text{coût}_{A}(x)$.

\section{Arbres binaires de recherche}

\begin{definition}[Arbre binaire]\index{Arbre binaire}
Un arbre binaire est soit vide, soit une racine avec un arbre binaire à gauche et un arbre binaire à droite.
\end{definition}

En python :

\lstinputlisting{Scripts/ArbreBinaire.py}

\begin{definition}[Arbre binaire de recherche]\index{Arbre binaire de recherche}
Un arbre binaire de recherche (ABR) est un arbre étiqueté avec les propriétés suivantes :

%Pour chaque sous-arbre $A^{\prime}$ : ???
\begin{enumerate}
\item $A.val$ est plus grand que toutes les valeurs de $A.gauche$.
\item $A.val$ est plus petit que toutes les valeurs de $A.droite$.
\end{enumerate}
\end{definition}

\begin{definition}[Taille d'un arbre]
On écrit $\vert A \vert$ la taille de l'arbre $A$ son nombre de sommets.

$ \vert A \vert = 
   \left \{
   \begin{array}{l}
      0 \text{ si l'arbre est vide} \\
      1 + \vert A.gauche \vert + \vert A.droit \vert \text{ sinon}
   \end{array}
   \right .
$
\end{definition}

\begin{definition}[Profondeur d'un nœud]\index{Profondeur d'un nœud (Arbre)}
On définit la profondeur d'un nœud $v$ dans l'arbre $A$ comme la longueur du chemin de la racine de $A$ jusqu'au sous arbre de racine $v$:\\[0.1cm]
$ \mathrm{Prof}_{A}(v) = 
   \left \{
   \begin{array}{l}
      0 \text{ si } A.val = v \\
      1 + \mathrm{Prof}_{A.gauche}(v) \text{ si } A.val > v \\
      1 + \mathrm{Prof}_{A.droite}(v) \text{ si } A.val < v
   \end{array}
   \right .
$
\end{definition}

\begin{definition}[Hauteur d'un arbre]\index{Hauteur (Arbre)}
On définit la hauteur d'un arbre $A$:

\vspace{0.2cm}

$ \mathrm{h}(A) = 
   \left \{
   \begin{array}{l}
      0 \text{ si l'arbre est vide} \\
      1 + \max ( h(A.gauche),\, h(A.droit) ) \text{ sinon}
   \end{array}
   \right .
$
\end{definition}
Remarque : $\mathrm{h}(A) = \max {\mathrm{Prof}_{A}(v),\, v \in A}$

\section{Recherche dans un ABR}\index{Recherche dans un ABR}
\lstinputlisting{Scripts/TrouverABR.py}

\begin{proposition}
$ T(A, v) = 
   \left \{
   \begin{array}{l l}
		\mathrm{Prof}_{A}(v) & \text{ si } v \in A \\
		\mathrm{h}(A) & \text{ si } v \notin A
   \end{array}
   \right .
$
\end{proposition}\label{pro:coutabr}

$\text{coût}_{\text{trouverABR}}(A,\, v) \leq T(A,\, v),
\forall \text{ ABR } A, \forall \text{ valeur } v, \vert A \vert \geq 1$

\subsection{Preuve par récurrence sur $\vert A \vert$}

\subsubsection*{Hypothèse d'induction}

$H(n) = $ l'énoncé de la proposition \ref{pro:coutabr} est vrai $\forall A,\, v$ $\vert A \vert \leq n$

\subsubsection*{Initialisation ("Base")}

$n = 1$ : \begin{minipage}[t]{\textwidth} $A$ a une seule valeur.

si $v \in A$ alors $v$ se trouve à la racine, \textcolor{ocre}{1 comparaison}.

si $v \notin A$ alors on fera \textcolor{ocre}{5 comparaisons}.

\end{minipage}

\subsubsection*{Hérédité ("Pas d'induction")}

Supposons $H(n)$ est vraie pour $n \leq 1$, montrons $H(n + 1)$.

Soit $A$ de taille $n + 1$.

Si $v \in A$ : \begin{minipage}[t]{\textwidth}

	Si $A.val = v$ on s'arrête après \textcolor{ocre}{1 comparaison}.
	
	Si $A.val > v$ : \begin{minipage}[t]{\textwidth}
		On fait \textcolor{ocre}{$ 3 + T(A.gauche,\, v) $ comparaisons (au plus)}
	
		$T(A.gauche,\, v) = 5 ( \mathrm{Prof}_{A.gauche}(v)$ par $H(\vert A.gauche \vert)$
		
		On doit montrer que $T(A.gauche,\, v) + 3 \leq T(A,\, v)$
		
		$\begin{array}{r c l}
			\Leftrightarrow 5 (\mathrm{Prof}_{A.gauche}(v) + 1) + 3 \leq T(A,\, v) & = & 5 (\mathrm{Prof}_{A}(v) + 1) \\
			\text{D'après la définition de } \mathrm{Prof}_{A}(v)\text{, cas } v < A.val & = & 5 (\mathrm{Prof}_{A.gauche}(v) + 1 + 1) \\
																				& = & 5 (\mathrm{Prof}_{A.gauche}(v) + 1) + 5
		\end{array}$
	\end{minipage}
	
	Si $A.val < v$ : \begin{minipage}[t]{\textwidth}
		On fait \textcolor{ocre}{$T(A.droite,\, v) + 5 $ comparaisons (au plus)}
		
		[...]
	\end{minipage}
\end{minipage}

Si $v \notin A$ : \begin{minipage}[t]{\textwidth}

	Si $A.val = v$, \textcolor{orange!90}{impossible}
	
	
	\begin{tabular}{c r c l}
	Si $A.val > v$ : & $\sharp comparaisons$ & $=$ & $T(A.gauche,\, v) + 3$ \\
					&  & $\leq$ & $5 \mathrm{h}(A.gauche) + 3 \,\text{ par hypothèse d'induction}$ \\
					&  & $\leq$ & $5 (\mathrm{h}(A) - 1) + 3 \,\text{ par définition de } \mathrm{h}$ \\
					&  & $\leq$ & $5 \mathrm{h}(A)$
		
	\end{tabular}
	
	\begin{tabular}{c r c l}
	Si $A.val < v$ : & $\sharp comparaisons$ & $=$ & $T(A.droit,\, v) + 5$ \\
					&  & $\leq$ & $5 \mathrm{h}(A)$
	\end{tabular}

\end{minipage}

\section{Insertion dans un ABR}\index{Insertion (ABR)}

\subsection{Algorithme}

\lstinputlisting{Scripts/InsererABR.py}

On souhaite faire l'analyse amortie de cet algorithme, en supposant qu'on insère les éléments $1,\, 2,\, ...\ N$ dans un ordre aléatoire.

$\text{coût }_{\text{Amorti}} = \sum\limits_{i = 1}^{N} \text{coût}_{\text{InsérerABR}} (\text{la i}^{\text{ème}}\text{ valeur})$

On suppose qu'il reste à insérer les valeurs $x_{1} < x_{2} < ... < x_{N}$ et on choisit d'insérer $x_{i}$ avec une probabilité de $\frac{1}{N}$.

Si on choisit $x_{i}$, on aura $i-1$ éléments à gauche et $n-i$ éléments à droite.\\

\subsection{Coût moyen d'insertion de $n$ éléments dans un ABR vide}

\begin{proposition}
Le coût moyen d'insérer $x_{1} < x_{2} < ... < x_{N}$ dans un ABR initialement vide, noté $C_{N}$ est : (où on compte le nombre d'appels récursifs)

\vspace{0.16cm} %WARNING
$ C_{N} = 
   \left \{
   \begin{array}{l}
      0 \text{ si } N = 0 \\
      1 \text{ si } N = 1 \\
      \sum\limits_{i = 1}^{N} \frac{1}{N}[ 1 \;+\ N-1 \;+\ C_{i-1} \;+\; C_{N-i} ] \text{ sinon }
   \end{array}
   \right .
$
\vspace{0.16cm}

Car : \begin{minipage}[t]{\textwidth}
	$\text{coût}(\text{placer } x_{i}) = 1$
	
	les $N - 1$ qui restent passent par la racine,
	
	$x_{1}\; ...\ x_{i-1}$ sont insérés à gauche,
	
	$x_{i+1}\; ...\ x_{N}$ sont insérés à droite.
\end{minipage}

\end{proposition}

\begin{proposition}
$C_{N} = O(N \log N)$
\end{proposition}

\begin{proof}
Prenons $N > 1$

\begin{tabular}{r c l}
	$C_{N}$ & $=$ & $N \;+\; \frac{1}{N} \sum\limits_{i = 1}^{N} ( C_{i-1} + C_{N - i} )$ \\
	
			& $=$ & $N \;+\; \frac{1}{N} (\sum\limits_{i = 1}^{N} C_{i-1} + \sum\limits_{j = 1}^{N} C_{N - j})$ \\
			
			& $=$ & $N \;+\; \frac{1}{N} \; 2 \sum\limits_{k = 0}^{N-1} C_{k}$

\end{tabular}

\begin{tabular}{l r c l}
	Télescopons : & $N C_{N}$ & $=$ & $N^{2} + 2 \sum\limits_{k = 0}^{N-1} C_{k}$ \\
	& $(N-1) C_{N-1}$ & $=$ & $(N-1)^{2} + 2 \sum\limits_{k = 0}^{N-2} C_{k}$ \\
	& $N C_{N} - (N-1) C_{N-1}$ & $=$ & $N^{2} - (N-1)^{2} + 2 C_{N-1}$ \\
	& $N C_{N}$ & $=$ & $2 N - 1 \,+\, (N+1) C_{N-1}$ \\
	& $\frac{C_{N}}{N+1}$ & $=$ & $\frac{2N - 1}{N(N+1)} \,+\, \frac{C_{N-1}}{N}$

\end{tabular}

On pose : $t(N) = \frac{2N-1}{N(N+1)} \simeq \frac{2}{N}$

$\begin{array}{c c l}
	\frac{C_{N}}{N+1} & = & t(N) + \frac{C_{N-1}}{N} \\
	\frac{C_{N}}{N+1} & = & t(N) + t(N-1) + \frac{C_{N-2}}{N-1} \\
					& = & t(N) + t(N-1) + t(N-2) + \frac{C_{N-3}}{N-2} \\
					& = & \sum\limits_{i = 2}^{N} t(i) + \frac{1}{2} \\
					& \approx & \sum\limits_{i = 1}^{N} \frac{2}{i} + \frac{1}{2} \\
					& \approx & 2 \sum\limits_{i = 1}^{N} \frac{1}{i}
\end{array}$

$\frac{C_{N}}{N+1} \approx 2 * N \ln n$

On conclut que $C_{N} = 2 (N+1) N \ln(N)$ et que le coût amorti $\frac{C_{N}}{N} = 2(N+1) \ln(N)$

\end{proof}
